import{h,f as m,i as b,j as f,R as l}from"./index-0L8iWxnJ.js";class y extends h{constructor(){super(...arguments),Object.defineProperty(this,"lc_namespace",{enumerable:!0,configurable:!0,writable:!0,value:["langchain","output_parsers","default"]}),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0})}static lc_name(){return"NoOpOutputParser"}parse(e){return Promise.resolve(e)}getFormatInstructions(){return""}}function g(t){return typeof t._llmType=="function"}function i(t){if(g(t))return t;if("bound"in t&&l.isRunnable(t.bound))return i(t.bound);if("runnable"in t&&"fallbacks"in t&&l.isRunnable(t.runnable))return i(t.runnable);if("default"in t&&l.isRunnable(t.default))return i(t.default);throw new Error("Unable to extract BaseLanguageModel from llmLike object.")}class o extends m{static lc_name(){return"LLMChain"}get inputKeys(){return this.prompt.inputVariables}get outputKeys(){return[this.outputKey]}constructor(e){if(super(e),Object.defineProperty(this,"lc_serializable",{enumerable:!0,configurable:!0,writable:!0,value:!0}),Object.defineProperty(this,"prompt",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"llm",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"llmKwargs",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),Object.defineProperty(this,"outputKey",{enumerable:!0,configurable:!0,writable:!0,value:"text"}),Object.defineProperty(this,"outputParser",{enumerable:!0,configurable:!0,writable:!0,value:void 0}),this.prompt=e.prompt,this.llm=e.llm,this.llmKwargs=e.llmKwargs,this.outputKey=e.outputKey??this.outputKey,this.outputParser=e.outputParser??new y,this.prompt.outputParser){if(e.outputParser)throw new Error("Cannot set both outputParser and prompt.outputParser");this.outputParser=this.prompt.outputParser}}getCallKeys(){return"callKeys"in this.llm?this.llm.callKeys:[]}_selectMemoryInputs(e){const r=super._selectMemoryInputs(e),u=this.getCallKeys();for(const a of u)a in e&&delete r[a];return r}async _getFinalOutput(e,r,u){let a;return this.outputParser?a=await this.outputParser.parseResultWithPrompt(e,r,u?.getChild()):a=e[0].text,a}call(e,r){return super.call(e,r)}async _call(e,r){const u={...e},a={...this.llmKwargs},p=this.getCallKeys();for(const s of p)s in e&&a&&(a[s]=e[s],delete u[s]);const n=await this.prompt.formatPromptValue(u);if("generatePrompt"in this.llm){const{generations:s}=await this.llm.generatePrompt([n],a,r?.getChild());return{[this.outputKey]:await this._getFinalOutput(s[0],n,r)}}const c=await(this.outputParser?this.llm.pipe(this.outputParser):this.llm).invoke(n,r?.getChild());return{[this.outputKey]:c}}async predict(e,r){return(await this.call(e,r))[this.outputKey]}_chainType(){return"llm"}static async deserialize(e){const{llm:r,prompt:u}=e;if(!r)throw new Error("LLMChain must have llm");if(!u)throw new Error("LLMChain must have prompt");return new o({llm:await f.deserialize(r),prompt:await b.deserialize(u)})}serialize(){const e="serialize"in this.llm?this.llm.serialize():void 0;return{_type:`${this._chainType()}_chain`,llm:e,prompt:this.prompt.serialize()}}_getNumTokens(e){return i(this.llm).getNumTokens(e)}}export{o as LLMChain};
